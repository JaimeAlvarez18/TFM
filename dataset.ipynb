{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import cv2\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "device ='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import gc         \n",
    "mp.set_start_method('spawn', force=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1169999, 130000, 50000, 1169999, 130000, 50000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=[]\n",
    "test=[]\n",
    "y_train=[]\n",
    "y_test=[]\n",
    "fichero='Datasets/GenImage/'\n",
    "itera=os.walk(fichero)\n",
    "datasets=next(iter(itera))[1]\n",
    "for index,dataset in enumerate(datasets):\n",
    "    direct=fichero+dataset+'/'\n",
    "    train.append(glob(direct+'train/ai/*.PNG')+glob(direct+'train/ai/*.png'))\n",
    "    test.append(glob(direct+'val/ai/*.PNG')+glob(direct+'val/ai/*.png'))\n",
    "    y_train.append([dataset]*len(train[index]))\n",
    "    y_test.append([dataset]*len(test[index]))\n",
    "\n",
    "train = [item for sublist in train for item in sublist]\n",
    "test = [item for sublist in test for item in sublist]\n",
    "y_train = [item for sublist in y_train for item in sublist]\n",
    "y_test = [item for sublist in y_test for item in sublist]\n",
    "\n",
    "label_encoder=LabelEncoder().fit(y_train)\n",
    "y_train=label_encoder.transform(y_train)\n",
    "y_test=label_encoder.transform(y_test)\n",
    "\n",
    "\n",
    "train,val,y_train,y_val = train_test_split(train,y_train,train_size=0.9,stratify=y_train,random_state=5)\n",
    "\n",
    "len(train),len(val),len(test),len(y_train),len(y_val),len(y_test)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 2/1:  18%|█▊        | 205758/1169999 [14:24<1:13:33, 218.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Image at Datasets/GenImage/BigGan/train/ai/116_biggan_00098.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 2/1:  25%|██▍       | 288709/1169999 [20:15<1:00:31, 242.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Image at Datasets/GenImage/ADM/train/ai/115_adm_156.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 2/1:  33%|███▎      | 390098/1169999 [27:19<52:50, 245.96it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Image at Datasets/GenImage/ADM/train/ai/115_adm_142.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 2/1:  33%|███▎      | 391316/1169999 [27:24<50:51, 255.16it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Image at Datasets/GenImage/ADM/train/ai/115_adm_135.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 2/1:  35%|███▌      | 413209/1169999 [28:56<52:43, 239.22it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Image at Datasets/GenImage/stable_diffusion_v_1_4/train/ai/033_sdv4_00134.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 2/1:  36%|███▌      | 417375/1169999 [29:13<47:50, 262.19it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Image at Datasets/GenImage/BigGan/train/ai/116_biggan_00107.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 2/1:  37%|███▋      | 431953/1169999 [30:14<48:56, 251.30it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Image at Datasets/GenImage/stable_diffusion_v_1_4/train/ai/033_sdv4_00152.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 2/1:  39%|███▊      | 452264/1169999 [31:39<43:47, 273.17it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Image at Datasets/GenImage/ADM/train/ai/115_adm_154.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 2/1:  49%|████▊     | 567738/1169999 [39:43<37:01, 271.06it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Image at Datasets/GenImage/Midjourney/train/ai/208_midjourney_76.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 2/1:  76%|███████▌  | 883479/1169999 [1:01:51<18:15, 261.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Image at Datasets/GenImage/BigGan/train/ai/116_biggan_00081.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 2/1:  80%|███████▉  | 935248/1169999 [1:05:29<15:51, 246.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Image at Datasets/GenImage/ADM/train/ai/115_adm_164.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 2/1:  82%|████████▏ | 963442/1169999 [1:07:30<16:24, 209.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Image at Datasets/GenImage/stable_diffusion_v_1_4/train/ai/033_sdv4_00137.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 2/1:  86%|████████▌ | 1006345/1169999 [1:10:30<11:38, 234.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Image at Datasets/GenImage/ADM/train/ai/115_adm_141.PNG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 2/1:  87%|████████▋ | 1014252/1169999 [1:11:04<10:38, 243.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Image at Datasets/GenImage/GLIDE/train/ai/GLIDE_1000_200_08_808_glide_00033.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 2/1:  93%|█████████▎| 1092293/1169999 [1:16:33<05:29, 236.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Image at Datasets/GenImage/Midjourney/train/ai/208_midjourney_91.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 2/1: 100%|██████████| 1169999/1169999 [1:22:00<00:00, 237.77it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(train, desc=f\"Validating Epoch {1 + 1}/{1}\"):\n",
    "    image1 = cv2.imread(i)\n",
    "    if type(image1) == type(None):\n",
    "            print(f\"Warning: Image at {i}\")\n",
    "            os.remove(i)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 2/1:  61%|██████    | 79144/130000 [05:43<03:41, 229.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Image at Datasets/GenImage/Midjourney/train/ai/208_midjourney_92.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 2/1: 100%|██████████| 130000/130000 [09:26<00:00, 229.41it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(val, desc=f\"Validating Epoch {1 + 1}/{1}\"):\n",
    "    image1 = cv2.imread(i)\n",
    "    if type(image1) == type(None):\n",
    "            print(f\"Warning: Image at {i}\")\n",
    "            os.remove(i)\n",
    "    del image1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 2/1: 100%|██████████| 50000/50000 [03:41<00:00, 225.44it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(test, desc=f\"Validating Epoch {1 + 1}/{1}\"):\n",
    "    image1 = cv2.imread(i)\n",
    "    if type(image1) == type(None):\n",
    "            print(f\"Warning: Image at {i}\")\n",
    "            os.remove(i)\n",
    "    del image1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pairs(x, y):\n",
    "    \"\"\"Creates a tuple containing image pairs with corresponding label.\n",
    "\n",
    "    Arguments:\n",
    "        x: List containing images, each index in this list corresponds to one image.\n",
    "        y: List containing labels, each label with datatype of `int`.\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing two numpy arrays as (pairs_of_samples, labels),\n",
    "        where pairs_of_samples' shape is (2len(x), 2,n_features_dims) and\n",
    "        labels are a binary array of shape (2len(x)).\n",
    "    \"\"\"\n",
    "\n",
    "    num_classes = max(y) + 1\n",
    "    digit_indices = [np.where(y == i)[0] for i in range(num_classes)] # 10 vectores con los índices de cada número\n",
    "\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    \n",
    "    for idx1 in range(len(x)):  # por cada muestra de x_train (o de x_test)\n",
    "        # add a matching example\n",
    "        x1 = x[idx1]   # muestra 0,1,2,... \n",
    "        label1 = y[idx1] # etiqueta correspondiente a la muestra 0,1,2,....\n",
    "        idx2 = random.choice(digit_indices[label1]) # escogemos al azar una muestra con la misma etiqueta (label1)\n",
    "        x2 = x[idx2]\n",
    "\n",
    "        pairs += [[x1, x2]]  # una muestra para Siamese Network, con un par de imágenes que representan el mismo número\n",
    "        labels += [0] # 0 aquí indica muestra con imágenes de la misma categoría\n",
    "\n",
    "        # add a non-matching example\n",
    "        label2 = random.randint(0, num_classes - 1)\n",
    "        while label2 == label1: # hasta que salga un número diferente a label1\n",
    "            label2 = random.randint(0, num_classes - 1)\n",
    "\n",
    "        idx2 = random.choice(digit_indices[label2]) # tomamos una muestra al azar de entre las imágenes del número label2\n",
    "        x2 = x[idx2]\n",
    "\n",
    "        pairs += [[x1, x2]] # una muestra para Siamese Network, con un par de imágenes que representan el mismo número\n",
    "        labels += [1] # 1 aquí indica muestra con imágenes de la misma categoría\n",
    "\n",
    "    return np.array(pairs), np.array(labels).astype(\"float32\") # vectores de muestras (de entrenamiento o testeo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2339968, 2), (2339968,), (259998, 2), (259998,), (100000, 2), (100000,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train,y_train=make_pairs(train, y_train)\n",
    "val,y_val=make_pairs(val, y_val)\n",
    "test,y_test=make_pairs(test, y_test)\n",
    "\n",
    "train.shape,y_train.shape,val.shape,y_val.shape,test.shape,y_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class images_Dataset(Dataset):\n",
    "    def __init__(self,route_images,classes,device):\n",
    "        self.route_images=route_images\n",
    "        self.classes=classes\n",
    "        self.device=device\n",
    "    def __len__(self):\n",
    "        return len(self.route_images)\n",
    "    def __getitem__(self, index):\n",
    "        im1,im2=self.route_images[index]\n",
    "        \n",
    "        y=self.classes[index]\n",
    "\n",
    "        #load images\n",
    "        image1 = cv2.imread(im1)\n",
    "        image2 = cv2.imread(im2)\n",
    "\n",
    "\n",
    "        if type(image1) == type(None):\n",
    "            print(f\"Warning: Image at {im1}\")\n",
    "            os.remove(im1)\n",
    "        \n",
    "        if  type(image2) == type(None):\n",
    "            print(f\"Warning: Image at {im2}\")\n",
    "            os.remove(im2)\n",
    "\n",
    "        #to RGB\n",
    "        try:\n",
    "            image1 = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n",
    "        except Exception as e:\n",
    "            print(im1)\n",
    "\n",
    "        try:\n",
    "            image2 = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)\n",
    "        except Exception as e:\n",
    "            print(im2)\n",
    "\n",
    "\n",
    "        \n",
    "        #scale images to [0,1]\n",
    "        image1=image1/255.0\n",
    "        image2=image2/255.0\n",
    "\n",
    "        #Rescale resolution to 256,256\n",
    "        image1 = cv2.resize(image1, (256, 256))\n",
    "        image2 = cv2.resize(image2, (256, 256))\n",
    "\n",
    "        #Permute dimensions\n",
    "        image1=np.transpose(image1,[2,0,1])\n",
    "        image2=np.transpose(image2,[2,0,1])\n",
    "\n",
    "        image1=torch.from_numpy(image1).to(self.device)\n",
    "        image2=torch.from_numpy(image2).to(self.device)\n",
    "\n",
    "        image1=image1.to(torch.float32)\n",
    "        image2=image2.to(torch.float32)\n",
    "\n",
    "        y=torch.tensor(y).to(torch.float32).to(self.device)\n",
    "\n",
    "        return image1,image2, y #devuelve los datos cargados y las clases\n",
    "\n",
    "train_dataset=images_Dataset(train[:50000],y_train[:50000],device)\n",
    "train_dataloader=DataLoader(train_dataset,batch_size=128,shuffle=True,num_workers=4)\n",
    "\n",
    "val_dataset=images_Dataset(val[:10000],y_val[:10000],device)\n",
    "val_dataloader=DataLoader(val_dataset,batch_size=64,shuffle=True,num_workers=4)\n",
    "\n",
    "test_dataset=images_Dataset(test[:5000],y_test[:5000],device)\n",
    "test_dataloader=DataLoader(test_dataset,batch_size=64,shuffle=True)\n",
    "\n",
    "del train,val,test,y_train,y_test,y_val,train_dataset,val_dataset,test_dataset\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/usr/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'images_Dataset' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m     plt\u001b[38;5;241m.\u001b[39mtight_layout()\n\u001b[1;32m     43\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m---> 45\u001b[0m a,b,c\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     46\u001b[0m a\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(a\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     47\u001b[0m b\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(b\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m/mnt/c/Users/jalva/Desktop/TFM/TFM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:491\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/jalva/Desktop/TFM/TFM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:422\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/jalva/Desktop/TFM/TFM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1146\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1139\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1141\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1146\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m/usr/lib/python3.12/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.12/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/multiprocessing/context.py:289\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentinel \u001b[38;5;241m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m, closefd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetbuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def visualize(images, labels):\n",
    "    \"\"\"Creates a plot of pairs and labels, and prediction if it's test dataset.\n",
    "\n",
    "    Arguments:\n",
    "        pairs: Numpy Array, of pairs to visualize, having shape\n",
    "               (Number of pairs, 2, 28, 28).\n",
    "        to_show: Int, number of examples to visualize (default is 6)\n",
    "                `to_show` must be an integral multiple of `num_col`.\n",
    "                 Otherwise it will be trimmed if it is greater than num_col,\n",
    "                 and incremented if if it is less then num_col.\n",
    "        num_col: Int, number of images in one row - (default is 3)\n",
    "                 For test and train respectively, it should not exceed 3 and 7.\n",
    "        predictions: Numpy Array of predictions with shape (to_show, 1) -\n",
    "                     (default is None)\n",
    "                     Must be passed when test=True.\n",
    "        test: Boolean telling whether the dataset being visualized is\n",
    "              train dataset or test dataset - (default False).\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    batch_size = images.shape[0]\n",
    "    fig, axes = plt.subplots(batch_size, 2, figsize=(10, 5 * batch_size))\n",
    "\n",
    "    # Loop through the batch and plot each pair of images\n",
    "    for i in range(batch_size):\n",
    "        # Get the pair of images for this batch\n",
    "        for j in range(2):  # Two images in each pair\n",
    "            image = images[i, j].transpose(1, 2, 0)  # Change shape from (channels, height, width) to (height, width, channels)\n",
    "\n",
    "            # Plot the image\n",
    "            ax = axes[i, j]\n",
    "            ax.imshow(image)\n",
    "            ax.axis('off')  # Remove axis for better visualization\n",
    "\n",
    "        # Add title with label at the top of each pair (above the two images)\n",
    "        ax_center = axes[i, 0]  # You can choose either of the axes in the pair, here I used the left image (axes[i, 0])\n",
    "        ax_center.text(1.07, 1, f'Label: {labels[i]}', color='black', fontsize=19, ha='center', va='bottom', transform=ax_center.transAxes)\n",
    "\n",
    "\n",
    "    # Adjust layout and show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "a,b,c=next(iter(train_dataloader))\n",
    "a=np.array(a.to('cpu'))\n",
    "b=np.array(b.to('cpu'))\n",
    "c=np.array(c.to('cpu'))\n",
    "\n",
    "visualize(np.array([a[:4,:,:,:],b[:4,:,:,:]]).transpose(1,0,2,3,4), c[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self,out1, out2, labels, margin=1.0):\n",
    "        \"\"\"\n",
    "        Contrastive loss function.\n",
    "        \n",
    "        Args:\n",
    "        - out1: The embedding of the first image in the pair.\n",
    "        - out2: The embedding of the second image in the pair.\n",
    "        - labels: A tensor of labels (0 for related, 1 for unrelated).\n",
    "        - margin: The margin that separates positive and negative pairs.\n",
    "        \n",
    "        Returns:\n",
    "        - The contrastive loss.\n",
    "        \"\"\"\n",
    "        # Compute the Euclidean distance between the embeddings\n",
    "\n",
    "        euclidean_distance = F.pairwise_distance(out1, out2, p=2)\n",
    "        \n",
    "        # Calculate the contrastive loss\n",
    "        loss = torch.mean((1 - labels) * torch.pow(euclidean_distance, 2) + \n",
    "                        (labels) * torch.pow(torch.clamp(margin - euclidean_distance, min=0.0), 2))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "class Contrastive_loss(nn.Module):\n",
    "    \"\"\"Provides 'contrastive_loss' an enclosing scope with variable 'margin'.\n",
    "\n",
    "    Arguments:\n",
    "        margin: Integer, defines the baseline for distance for which pairs\n",
    "                should be classified as dissimilar. - (default is 1).\n",
    "\n",
    "    Returns:\n",
    "        'contrastive_loss' function with data ('margin') attached.\n",
    "    \"\"\"\n",
    "\n",
    "    # Contrastive loss = mean( (1-true_value) * square(prediction) +\n",
    "    #                         true_value * square( max(1-prediction, 0) ))\n",
    "    def __init__(self,margin=1):\n",
    "        super(Contrastive_loss,self).__init__()\n",
    "        self.margin=margin\n",
    "\n",
    "        \n",
    "    def forward(self,y_true, y_pred):\n",
    "        \"\"\"Calculates the contrastive loss.\n",
    "\n",
    "        Arguments:\n",
    "            y_true: List of labels, each label is of type float32.\n",
    "            y_pred: List of predictions of same length as of y_true,\n",
    "                    each label is of type float32.\n",
    "\n",
    "        Returns:\n",
    "            A tensor containing contrastive loss as floating point value.\n",
    "        \"\"\"\n",
    "\n",
    "        # square_pred = torch.square(y_pred)\n",
    "\n",
    "        # # Calculate the margin squared term\n",
    "        # margin_square = torch.square(torch.maximum(self.margin - y_pred, torch.tensor(0.0)))\n",
    "\n",
    "        # # Contrastive loss formula\n",
    "        # loss = torch.mean((1 - y_true) * square_pred + y_true * margin_square)\n",
    "\n",
    "        # return loss\n",
    "        loss = torch.mean((1 - y_true) * torch.square(y_pred) + y_true * torch.square(torch.maximum(self.margin - y_pred, torch.zeros_like(y_pred))))\n",
    "        return loss\n",
    "class CLIP_Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(emb_im1, emb_im2,temperature=1.0):\n",
    "        # logits[i][j] is the dot_similarity(caption_i, image_j).\n",
    "        logits = torch.matmul(emb_im1, emb_im2.T) / temperature\n",
    "\n",
    "        # images_similarity[i][j] is the dot_similarity(image_i, image_j).\n",
    "        images_similarity = torch.matmul(emb_im2, emb_im2.T)\n",
    "\n",
    "        # captions_similarity[i][j] is the dot_similarity(caption_i, caption_j).\n",
    "        captions_similarity = torch.matmul(emb_im1, emb_im1.T)\n",
    "\n",
    "        # targets[i][j] = average dot_similarity(caption_i, caption_j) and dot_similarity(image_i, image_j).\n",
    "        targets = F.softmax((captions_similarity + images_similarity) / (2 * temperature), dim=-1)\n",
    "\n",
    "        # Compute the loss for the captions using crossentropy\n",
    "        captions_loss = F.cross_entropy(logits, targets.argmax(dim=-1), reduction='mean')\n",
    "\n",
    "        # Compute the loss for the images using crossentropy\n",
    "        images_loss = F.cross_entropy(logits.T, targets.argmax(dim=-1), reduction='mean')\n",
    "\n",
    "        # Return the mean of the loss over the batch.\n",
    "        return (captions_loss + images_loss) / 2\n",
    "\n",
    "\n",
    "    \n",
    "class siamese_model(nn.Module):\n",
    "    def __init__(self,type,device):\n",
    "        super().__init__()\n",
    "        self.device=device\n",
    "        self.type=type\n",
    "        self.efficientNet = EfficientNet.from_name(type,3)  # Initialize from scratch\n",
    "        # Modify the classifier (output layer) of EfficientNet\n",
    "\n",
    "        self.efficientNet._fc = nn.Linear(1280,512)\n",
    "        self.norm=nn.BatchNorm1d(512)\n",
    "\n",
    "        # self.linear=nn.Linear(1280*2,2)\n",
    "        # self.softmax=nn.Softmax(dim=1)\n",
    "        \n",
    "\n",
    "    def euclidean_distance(self,vect1,vect2):\n",
    "        \"\"\"Find the Euclidean distance between two vectors.\n",
    "\n",
    "        Arguments:\n",
    "            vects: List containing two tensors of same length.\n",
    "\n",
    "        Returns:\n",
    "            Tensor containing euclidean distance\n",
    "            (as floating point value) between vectors.\n",
    "        \"\"\"\n",
    "\n",
    "        sum_square = torch.sum(torch.square(vect1 - vect2), dim=1, keepdim=True)\n",
    "        \n",
    "        # Return the square root of the sum of squares with numerical stability\n",
    "        epsilon = torch.tensor(torch.finfo(vect1.dtype).eps)  # Small value to avoid sqrt(0)\n",
    "        return torch.sqrt(torch.maximum(sum_square, epsilon))\n",
    "\n",
    "    def forward(self,im1,im2):\n",
    "        emb1=self.efficientNet(im1)\n",
    "        emb2=self.efficientNet(im2)\n",
    "        del im2, im1\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        emb1=self.norm(emb1)\n",
    "        emb2=self.norm(emb2)\n",
    "\n",
    "        # embs=torch.concat([emb1,emb2],dim=1).to(self.device)\n",
    "        # out=self.linear(embs)\n",
    "        # out=self.softmax(out)\n",
    "        # del embs,emb1,emb2\n",
    "        # gc.collect()\n",
    "        # torch.cuda.empty_cache()\n",
    "        return emb1,emb2\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=siamese_model('efficientnet-b0',device).to(device)\n",
    "criterion = ContrastiveLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/100:   0%|          | 0/391 [00:00<?, ?it/s]Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/usr/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'images_Dataset' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Training Epoch 1/100:   0%|          | 0/391 [2:13:50<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimage1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTraining Epoch \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mEPOCHS\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Forward pass\u001b[39;49;00m\n",
      "File \u001b[0;32m/mnt/c/Users/jalva/Desktop/TFM/TFM/.venv/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m/mnt/c/Users/jalva/Desktop/TFM/TFM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:491\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/jalva/Desktop/TFM/TFM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:422\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/jalva/Desktop/TFM/TFM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1146\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1139\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1141\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1146\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m/usr/lib/python3.12/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.12/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/multiprocessing/context.py:289\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentinel \u001b[38;5;241m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m, closefd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetbuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS=100\n",
    "train_loss=[]\n",
    "train_accuracy=[]\n",
    "best=9999999.0\n",
    "val_loss=[]\n",
    "val_accuracy=[]\n",
    "for epoch in range(EPOCHS):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Training loop\n",
    "    for image1, image2, label in tqdm(train_dataloader, desc=f\"Training Epoch {epoch + 1}/{EPOCHS}\"):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        pred1,pred2 = model(image1, image2)\n",
    "\n",
    "        del image1,image2\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # _,pred=torch.max(pred,1)\n",
    "        # pred = pred.detach().to(torch.float).requires_grad_(True).to(device)\n",
    "\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(pred1,pred2,label)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track running loss and accuracy\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # total += label.size(0)\n",
    "        # correct += (pred == label).sum().item()\n",
    "        del label, loss,pred1,pred2\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Calculate training loss and accuracy\n",
    "    train_loss_value = running_loss / len(train_dataloader)\n",
    "    # train_accuracy_value = 100 * correct / total\n",
    "\n",
    "    train_loss.append(train_loss_value)\n",
    "    # train_accuracy.append(train_accuracy_value)\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize validation stats\n",
    "    running_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    # Validation loop\n",
    "    with torch.no_grad():\n",
    "        for image1, image2, label in tqdm(val_dataloader, desc=f\"Validating Epoch {epoch + 1}/{EPOCHS}\"):\n",
    "\n",
    "            # Forward pass\n",
    "            pred1,pred2 = model(image1, image2)\n",
    "            del image1,image2\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # _,pred=torch.max(pred,1)\n",
    "            # pred = pred.detach().to(torch.float).requires_grad_(True).to(device)\n",
    "\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(pred1,pred2,label)\n",
    "\n",
    "            # Track running loss and accuracy\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            val_total += label.size(0)\n",
    "            # val_correct += (pred == label).sum().item()\n",
    "            del pred1,pred2,label,loss\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # Calculate validation loss and accuracy\n",
    "    val_loss_value = running_loss / len(val_dataloader)\n",
    "    # val_accuracy_value = 100 * val_correct / val_total\n",
    "\n",
    "    val_loss.append(val_loss_value)\n",
    "    # val_accuracy.append(val_accuracy_value)\n",
    "\n",
    "    if val_loss_value <= best:\n",
    "        best=val_loss_value\n",
    "        checkpoint = {\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            }\n",
    "        torch.save(checkpoint, 'Contrastive_model.pth')\n",
    "\n",
    "    print()\n",
    "    print('-'*60)\n",
    "    # Print results for the epoch\n",
    "    print(f\"Epoch [{epoch + 1}/{EPOCHS}]\")\n",
    "    print(f\"Train Loss: {train_loss_value:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss_value:.4f}\")\n",
    "    print('-'*60)\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
